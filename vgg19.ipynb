{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"vgg19.ipynb","provenance":[],"authorship_tag":"ABX9TyNdhionlLmkoR3eUq+5q8bA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"YAe086FDSSud"},"source":["import tensorflow as tf\n","from keras.applications.vgg19 import VGG19  \n","from keras.layers import Dense, Flatten\n","from keras.models import Model\n","from keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.callbacks import EarlyStopping \n","from glob import glob\n","import pickle\n","\n","\n","train_path = 'data_new'\n","valid_path = 'valid'\n","\n","\n","def create_model():\n","    # add preprocessing layer to the front of VGG\n","    vgg = VGG19(input_shape=[224, 224] + [3], weights='imagenet', include_top=False)\n","\n","    # don't train existing weights\n","    a = 0\n","    for layer in vgg.layers:\n","        if a== 0:\n","            layer.name=\"input\"\n","            a+=1\n","        layer.trainable = False\n","        \n","    # get number of classes\n","    folders = glob('valid/*')\n","      \n","    # new layers added\n","    x = Flatten()(vgg.output)\n","    \n","    layer = Dense(len(folders), activation='softmax', name='dense')(x)\n","    \n","    # create a model object\n","    model = Model(inputs=vgg.input, outputs=layer)\n","     \n","    # model.summary()\n","    \n","    # tell the model what cost and optimization method to use\n","    model.compile(\n","      loss='categorical_crossentropy',\n","      optimizer='adam',\n","      metrics=['acc']\n","    )\n","    return model\n","\n","\n","model = create_model()\n","\n","# Use the Image Data Generator to import the images from the dataset\n","\n","train_datagen = ImageDataGenerator(zoom_range = 0.2, \n","                                   rescale = 1./255,\n","                                   shear_range = 0.2,\n","                                   horizontal_flip = True)\n","\n","test_datagen = ImageDataGenerator(rescale = 1./255)\n","\n","training_set = train_datagen.flow_from_directory(train_path,\n","                                                 target_size = (224, 224),\n","                                                 batch_size = 32,\n","                                                 class_mode = 'categorical')\n","\n","test_set = test_datagen.flow_from_directory(valid_path,\n","                                            target_size = (224, 224),\n","                                            batch_size = 32,\n","                                            class_mode = 'categorical')\n","\n","earlystop = EarlyStopping(monitor='val_acc', \n","                          min_delta=0, #threshold\n","                          patience=5, \n","                          verbose=0, # what to print, default (0)\n","                          mode='auto') #direction of monitored quantity (min) for loss\n","\n","# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=2, \n","#                                             verbose=1, factor=0.5, min_lr=0.00001)\n","# filepath = '/content/drive/My Drive'\n","# callbacks=[ModelCheckpoint(filepath, monitor=‘val_acc’, save_best_only=True)]\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"out/ck/cp.ckpt\",\n","                                                 save_weights_only=True,\n","                                                 period=2,\n","                                                 verbose=1)\n","callbacks = [cp_callback, earlystop]\n","r = model.fit_generator(training_set, \n","                        validation_data=test_set,\n","                        steps_per_epoch=len(training_set),\n","                        epochs=100,\n","                        validation_steps=len(test_set), \n","                        callbacks=callbacks) \n","\n","#accuracies\n","plt.plot(r.history['acc'], label='train acc')\n","plt.plot(r.history['val_acc'], label='val acc')\n","plt.legend()\n","plt.show()\n","plt.savefig('out/AccVal_acc')\n","\n","# loss\n","plt.plot(r.history['loss'], label='train loss')\n","plt.plot(r.history['val_loss'], label='val loss')\n","plt.legend()\n","plt.show()\n","plt.savefig('out/LossVal_loss')\n","\n","\n","model.save('out/model_1.h5')\n","model.save('out/Model_1')      \n","model.save_weights('out/weights_model_1.h5')  \n","\n","with open('out/model.json', 'w') as f:\n","    f.write(model.to_json())\n"," \n","file = open( \"out/model_1_acc\",'wb') \n","pickle.dump(list(r.history['acc']), file) \n","file.close()\n","\n","file = open( \"out/model_1_loss\",'wb') \n","pickle.dump(list(r.history['loss']), file) \n","file.close()  \n","\n","file = open( \"out/model_1_val_acc\",'wb') \n","pickle.dump(list(r.history['val_acc']), file) \n","file.close() \n","\n","file = open( \"out/model_1_val_loss\",'wb') \n","pickle.dump(list(r.history['val_loss']), file) \n","file.close() "],"execution_count":null,"outputs":[]}]}